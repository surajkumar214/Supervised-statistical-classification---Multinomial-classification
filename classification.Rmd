---
title: 'Supervised statistical classification-Multinomial'
author: "Suraj Kumar"
date: "15/09/2021"
output: github_document
number_sections: yes
always_allow_html: true

fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(moderndive)
library(skimr)
library(kableExtra)
library(gridExtra)
library(GGally)
library(infer)
library(broom)
library(ggfortify)
library(jtools)
library(AER)
library(car)
library(janitor)
library(plotly)
library(nnet)
library(rpart)
library(rpart.plot)
library(randomForest)
library(neuralnet)
library(nnet)
library(VGAM)
library(NeuralNetTools)
```


```{r dataset, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
ablone<-read.csv("/home/suraj/Desktop/stats_folder/Supervised/Abalone.txt")
#loaded the dataset
ablone$sex <- as.factor(ablone$sex)
ablone$rings <- as.factor(ablone$rings)

car<-read.csv("/home/suraj/Desktop/stats_folder/Supervised/Car Evaluation.txt")
#loaded the dataset

car[sapply(car,is.character)]<-lapply(car[sapply(car,is.character)],as.factor)
#modified character variable to factor
contra <- read.csv("/home/suraj/Desktop/stats_folder/Supervised/Contraceptive Method Choice.txt")
contra[sapply(contra,is.character)]<-lapply(contra[sapply(contra,is.character)],as.factor)

nursery <- read.csv("/home/suraj/Desktop/stats_folder/Supervised/Nursery.txt")
nursery[sapply(nursery,is.character)]<-lapply(nursery[sapply(nursery,is.character)],as.factor)
```



```{r Exploratory & formal analysis ablone, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
skim(ablone)

ablone %>%
  ggplot(aes(x = rings,y = length),color = sex) +
  geom_boxplot() +
  facet_wrap("sex")
  labs(x = "ablone rings", y = "length",title="Boxplot of ablone length vs rings")
  
  
ablone %>%
  ggplot(aes(x = rings,y = ablone$diameter),color = sex) +
  geom_boxplot() +
  facet_wrap("sex")
  labs(x = "ablone rings", y = "length",title="Boxplot of ablone length vs rings")
  
ablone %>%
  ggplot(aes(x = rings,y = ablone$height),color = sex) +
  geom_boxplot() +
  facet_wrap("sex")
  labs(x = "ablone rings", y = "length",title="Boxplot of ablone length vs rings")
  
ablone %>%
  ggplot(aes(x = rings,y = ablone$whole.weight),color = sex) +
  geom_boxplot() +
  facet_wrap("sex")
  labs(x = "ablone rings", y = "length",title="Boxplot of ablone length vs rings")
  
ablone %>%
  ggplot(aes(x = rings,y = ablone$shucked.weight),color = sex) +
  geom_boxplot() +
  facet_wrap("sex")
  labs(x = "ablone rings", y = "length",title="Boxplot of ablone length vs rings")

ablone %>%
  ggplot(aes(x = rings,y = ablone$viscera.weight),color = sex) +
  geom_boxplot() +
  facet_wrap("sex")
  labs(x = "ablone rings", y = "length",title="Boxplot of ablone length vs rings")
  
ablone %>%
  ggplot(aes(x = rings,y = ablone$shell.weight),color = sex) +
  geom_boxplot() +
  facet_wrap("sex")
  labs(x = "ablone rings", y = "length",title="Boxplot of ablone length vs rings")
  
 

  
ablone %>%
  tabyl(rings,sex) %>%
  adorn_percentages() %>%
  adorn_pct_formatting() %>%
  adorn_ns()

ablone %>%
  select(-c(sex,rings))%>%
  ggpairs()

        


#No missing data

#addresesing outliers
plot<-ablone %>%
  select(-c(sex,rings))%>%
  ggplot(aes(x = length,y = height)) +
  geom_point() 
  
 
sp<-ggplotly(plot)

outliers <- which(ablone$height >=0.500)



ablone.upd<- ablone %>%
  slice(-outliers)


ablone.upd %>%
  select(-c(sex,rings))%>%
  ggpairs()



#dividing the data set into training and testing sub datasets

n <- dim(ablone.upd)[1]
set.seed(1)
ind <- sample(c(1:n),0.6*n) # 60%-40% training and test split
ablone.train <- ablone.upd[ ind,] 
ablone.test <- ablone.upd[-ind,]


#high multicolinearity, apply pca
ablone.train %>%
  select(-c(sex,rings))%>%
  summarise_if(is.numeric,
               list(v= var),na.rm = T)
  
ablone.pca <-ablone.train %>%
  select(-c(sex,rings))%>% 
  princomp(cor=T)

summary(ablone.pca)

biplot(ablone.pca,cex=.7,choices = c(1,4))

m1 <- multinom( rings ~ .,data = ablone.train,maxit = 1000)

summary(m1)


m2 <- multinom( ablone.train$rings~ablone.pca$scores[,1] + ablone.train$sex,maxit = 1000)

summary(m2)

#performance checking

sum(as.character(ablone.test$rings)!=as.character(predict(m1,ablone.test)))*100/dim(ablone.test)[1]

sum(as.character(ablone.train$rings)!=as.character(predict(m2)))*100/dim(ablone.train)[1]

#apply tree classification
#out of bag estimate, randomforest
raf <- randomForest(rings~., data = ablone.upd, ntree = 200)
print(raf)
importance(raf)
varImpPlot(raf)
ablone.train <- droplevels(ablone.train)

raf <- randomForest(ablone.train$rings~ablone.pca$scores[,1]+ablone.train$sex ,ntree = 200)
print(raf)
importance(raf)
varImpPlot(raf)

#, apply neural network
# have training and test data set
set.seed(84)
maxs <- apply(ablone.upd[,2:8], 2, max)
mins <- apply(ablone.upd[,2:8], 2, min)
ablone.upd[,2:8]<-scale(ablone.upd[,2:8], center = mins, scale = maxs - mins)
alpha <- 0.7 # percentage of training set
inTrain <- sample(1:n, round(alpha * n))
train.set <- ablone.upd[ inTrain,]
test.set <- ablone.upd[-inTrain,]

trainData <- cbind(class.ind(train.set$sex),train.set[, 2:8], class.ind(train.set$rings))
head(trainData)

testData <- cbind(class.ind(test.set$sex),test.set[, 2:8], class.ind(test.set$rings))


set.seed(84)

nn_ablone <- neuralnet(  `5`+ `6` + `7` + `8` + `9` + `10` + `11` + `12` + `13` + `14` + `15` + `16` + `17` + `18` + `19` + `20`   ~ F + I + M + length + diameter + height + whole.weight + shucked.weight + viscera.weight + shell.weight,data=trainData,hidden = c(2) ,linear.output=FALSE,err.fct='ce')


plot(nn_ablone)


original_values <- max.col(trainData[,c(15:30)])
predictions <- max.col(nn_ablone$net.result[[1]])
mean(original_values==predictions)

compute_test <- neuralnet::compute(nn_ablone,testData[, 1:10])
predictions_test <- max.col(compute_test$net.result)
original_values <- max.col(testData[,c(15:30)])
mean(original_values==predictions_test)
###################finished with ablone dataset######################
```



```{r Exploratory & formal analysis car , echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
skim(car)




tab2 <- table(car$class)
tab2<- as.data.frame(tab2)
colnames(tab2) <- c("class", "number")

ggplot(data = tab2, mapping = aes(x =class, y = number)) +
  geom_col()

 class.buying<- table(car$class,car$Buying)
 class.buying <- as.data.frame( class.buying)
 colnames(class.buying) <- c("class", "buying", "number")

 ggplot(data = class.buying, mapping = aes(x = buying, y = number, fill = class)) +
  geom_col() +
  labs(x = "buying", y = "class",
        title = "class distribution buying price wise") 

ggplot(data = class.buying, mapping = aes(x = buying, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "buying", y = "class",
        title = "class distribution buying price wise") 

ggplot(data = class.buying, mapping = aes(x = buying, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "buying", y = "class",
        title = "class distribution buying price wise") 

#second var maint
  class.maint<- table(car$class,car$maint)
 class.maint <- as.data.frame( class.maint)
 colnames(class.maint) <- c("class", "maint", "number")

 ggplot(data = class.maint, mapping = aes(x = maint, y = number, fill = class)) +
  geom_col() +
  labs(x = "maint", y = "class",
        title = "class distribution maintainence price wise") 

ggplot(data = class.maint, mapping = aes(x = maint, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "maint", y = "class",
        title = "class distribution maint price wise") 

ggplot(data = class.maint, mapping = aes(x = maint, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "maint", y = "class",
        title = "class distribution doors number wise") 

  #third variable doors
  class.doors<- table(car$class,car$doors)
 class.doors <- as.data.frame( class.doors)
 colnames(class.doors) <- c("class", "doors", "number")

 ggplot(data = class.doors, mapping = aes(x = doors, y = number, fill = class)) +
  geom_col() +
  labs(x = "doors", y = "class",
        title = "class distribution doors number wise") 

ggplot(data = class.doors, mapping = aes(x = doors, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "doors", y = "class",
        title = "class distribution doors number wise") 

ggplot(data = class.doors, mapping = aes(x = doors, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "doors", y = "class",
        title = "class distribution doors number wise") 


#fourth variable persons
  class.persons<- table(car$class,car$persons)
 class.persons <- as.data.frame( class.persons)
 colnames(class.persons) <- c("class", "persons", "number")

 ggplot(data = class.persons, mapping = aes(x = persons, y = number, fill = class)) +
  geom_col() +
  labs(x = "persons", y = "class",
        title = "class distribution persons number wise") 

ggplot(data = class.persons, mapping = aes(x = persons, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "persons", y = "class",
        title = "class distribution persons number wise") 

ggplot(data = class.persons, mapping = aes(x = persons, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "persons", y = "class",
        title = "class distribution persons number wise") 
  
#5th variable  persons
  class.lug<- table(car$class,car$lug.boot)
 class.lug <- as.data.frame( class.lug)
 colnames(class.lug) <- c("class", "lug", "number")

 ggplot(data = class.lug, mapping = aes(x = lug, y = number, fill = class)) +
  geom_col() +
  labs(x = "lug", y = "class",
        title = "class distribution lug space wise") 

ggplot(data = class.lug, mapping = aes(x = lug, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "lug", y = "class",
        title = "class distribution lug space wise") 

ggplot(data = class.lug, mapping = aes(x = lug, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "lug", y = "class",
        title = "class distribution lug space wise") 
  
  #6th var safety
   class.safety<- table(car$class,car$safety)
 class.safety <- as.data.frame( class.safety)
 colnames(class.safety) <- c("class", "safety", "number")

 ggplot(data = class.safety, mapping = aes(x = safety, y = number, fill = class)) +
  geom_col() +
  labs(x = "safety", y = "class",
        title = "class distribution safety wise") 

ggplot(data = class.safety, mapping = aes(x = safety, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "safety", y = "class",
        title = "class distribution safety wise") 

ggplot(data = class.safety, mapping = aes(x = safety, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "safety", y = "class",
        title = "class distribution safety wise") 
  #start formal analysis now
  
  
  # nueral network,svm
  #apply multinomial
  
  #dividing the data set into training and testing sub datasets

n <- dim(car)[1]
set.seed(1)
ind <- sample(c(1:n),0.6*n) # 60%-40% training and test split
car.train <- car[ ind,] 
car.test <- car[-ind,]

c1 <- multinom( class ~ .,data = car.train,maxit = 1000)

summary(c1)

mean(car.train$class==predict(c1))
mean(car.test$class==predict(c1,car.test))


#apply randomforest
car.raf <- randomForest(class~., data = car.train, ntree = 200)
print(car.raf)
importance(car.raf)
varImpPlot(car.raf)

mean(car.train$class==predict(car.raf))
mean(car.test$class==predict(car.raf,car.test))

#apply  neural network

car_matrix <- model.matrix(~Buying+maint+doors+persons+lug.boot+safety, data=car.train)
car_matrix <- car_matrix[,-1]


f <- paste(colnames(car_matrix_final[,1:15]),collapse="+")
trainData <- cbind(class.ind(car.train$class),car_matrix)
head(trainData)

#best 434, 442, 542,554,534,535,443,344,354**,353***,343**,334**,
set.seed(81)
nn_car1 <- neuralnet( acc+good+unacc+vgood~.,data = trainData ,hidden = c(3,5,3) ,linear.output=FALSE,err.fct='ce')


plot(nn_car1)

original_values <- max.col(trainData[,c(1:4)])
predictions <- max.col(nn_car1$net.result[[1]])
mean(original_values==predictions)
#prepare test data
car_matrix1 <- model.matrix(~Buying+maint+doors+persons+lug.boot+safety, data=car.test)
car_matrix1 <- car_matrix1[,-1]
testData <- cbind(class.ind(car.test$class),car_matrix1)

compute_test <- neuralnet::compute(nn_car1,testData[, 5:19])
predictions_test <- max.col(compute_test$net.result)
original_values <- max.col(testData[,c(1:4)])
mean(original_values==predictions_test)

  
```

```{r Exploratory & formal analysis nursery, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
skim(car)




tab2 <- table(car$class)
tab2<- as.data.frame(tab2)
colnames(tab2) <- c("class", "number")

ggplot(data = tab2, mapping = aes(x =class, y = number)) +
  geom_col()

 class.buying<- table(car$class,car$Buying)
 class.buying <- as.data.frame( class.buying)
 colnames(class.buying) <- c("class", "buying", "number")

 ggplot(data = class.buying, mapping = aes(x = buying, y = number, fill = class)) +
  geom_col() +
  labs(x = "buying", y = "class",
        title = "class distribution buying price wise") 

ggplot(data = class.buying, mapping = aes(x = buying, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "buying", y = "class",
        title = "class distribution buying price wise") 

ggplot(data = class.buying, mapping = aes(x = buying, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "buying", y = "class",
        title = "class distribution buying price wise") 

#second var maint
  class.maint<- table(car$class,car$maint)
 class.maint <- as.data.frame( class.maint)
 colnames(class.maint) <- c("class", "maint", "number")

 ggplot(data = class.maint, mapping = aes(x = maint, y = number, fill = class)) +
  geom_col() +
  labs(x = "maint", y = "class",
        title = "class distribution maintainence price wise") 

ggplot(data = class.maint, mapping = aes(x = maint, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "maint", y = "class",
        title = "class distribution maint price wise") 

ggplot(data = class.maint, mapping = aes(x = maint, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "maint", y = "class",
        title = "class distribution doors number wise") 

  #third variable doors
  class.doors<- table(car$class,car$doors)
 class.doors <- as.data.frame( class.doors)
 colnames(class.doors) <- c("class", "doors", "number")

 ggplot(data = class.doors, mapping = aes(x = doors, y = number, fill = class)) +
  geom_col() +
  labs(x = "doors", y = "class",
        title = "class distribution doors number wise") 

ggplot(data = class.doors, mapping = aes(x = doors, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "doors", y = "class",
        title = "class distribution doors number wise") 

ggplot(data = class.doors, mapping = aes(x = doors, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "doors", y = "class",
        title = "class distribution doors number wise") 


#fourth variable persons
  class.persons<- table(car$class,car$persons)
 class.persons <- as.data.frame( class.persons)
 colnames(class.persons) <- c("class", "persons", "number")

 ggplot(data = class.persons, mapping = aes(x = persons, y = number, fill = class)) +
  geom_col() +
  labs(x = "persons", y = "class",
        title = "class distribution persons number wise") 

ggplot(data = class.persons, mapping = aes(x = persons, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "persons", y = "class",
        title = "class distribution persons number wise") 

ggplot(data = class.persons, mapping = aes(x = persons, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "persons", y = "class",
        title = "class distribution persons number wise") 
  
#5th variable  persons
  class.lug<- table(car$class,car$lug.boot)
 class.lug <- as.data.frame( class.lug)
 colnames(class.lug) <- c("class", "lug", "number")

 ggplot(data = class.lug, mapping = aes(x = lug, y = number, fill = class)) +
  geom_col() +
  labs(x = "lug", y = "class",
        title = "class distribution lug space wise") 

ggplot(data = class.lug, mapping = aes(x = lug, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "lug", y = "class",
        title = "class distribution lug space wise") 

ggplot(data = class.lug, mapping = aes(x = lug, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "lug", y = "class",
        title = "class distribution lug space wise") 
  
  #6th var safety
   class.safety<- table(car$class,car$safety)
 class.safety <- as.data.frame( class.safety)
 colnames(class.safety) <- c("class", "safety", "number")

 ggplot(data = class.safety, mapping = aes(x = safety, y = number, fill = class)) +
  geom_col() +
  labs(x = "safety", y = "class",
        title = "class distribution safety wise") 

ggplot(data = class.safety, mapping = aes(x = safety, y = number, fill = class)) +
  geom_col(position = "dodge") +
  labs(x = "safety", y = "class",
        title = "class distribution safety wise") 

ggplot(data = class.safety, mapping = aes(x = safety, y = number, fill = class)) +
  geom_col() +
  facet_wrap(~class,ncol = 1)
  labs(x = "safety", y = "class",
        title = "class distribution safety wise") 
  #start formal analysis now
  
  
  # nueral network,svm
  #apply multinomial
  
  #dividing the data set into training and testing sub datasets

n <- dim(car)[1]
set.seed(1)
ind <- sample(c(1:n),0.6*n) # 60%-40% training and test split
car.train <- car[ ind,] 
car.test <- car[-ind,]

c1 <- multinom( class ~ .,data = car.train,maxit = 1000)

summary(c1)

mean(car.train$class==predict(c1))
mean(car.test$class==predict(c1,car.test))


#apply randomforest
car.raf <- randomForest(class~., data = car.train, ntree = 200)
print(car.raf)
importance(car.raf)
varImpPlot(car.raf)

mean(car.train$class==predict(car.raf))
mean(car.test$class==predict(car.raf,car.test))

#apply  neural network

car_matrix <- model.matrix(~Buying+maint+doors+persons+lug.boot+safety, data=car.train)
car_matrix <- car_matrix[,-1]


f <- paste(colnames(car_matrix_final[,1:15]),collapse="+")
trainData <- cbind(class.ind(car.train$class),car_matrix)
head(trainData)

#best 434, 442, 542,554,534,535,443,344,354**,353***,343**,334**,
set.seed(81)
nn_car1 <- neuralnet( acc+good+unacc+vgood~.,data = trainData ,hidden = c(3,5,3) ,linear.output=FALSE,err.fct='ce')


plot(nn_car1)

original_values <- max.col(trainData[,c(1:4)])
predictions <- max.col(nn_car1$net.result[[1]])
mean(original_values==predictions)
#prepare test data
car_matrix1 <- model.matrix(~Buying+maint+doors+persons+lug.boot+safety, data=car.test)
car_matrix1 <- car_matrix1[,-1]
testData <- cbind(class.ind(car.test$class),car_matrix1)

compute_test <- neuralnet::compute(nn_car1,testData[, 5:19])
predictions_test <- max.col(compute_test$net.result)
original_values <- max.col(testData[,c(1:4)])
mean(original_values==predictions_test)

  
```

